{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddd06f7-3837-4ef9-8d70-cfe075736595",
   "metadata": {},
   "source": [
    "# IST769 Final Exam\n",
    "\n",
    "**INSTRUCTIONS FOR HIGHEST GRADE POSSIBLE**\n",
    "\n",
    "Unless you are explicitly instructed otherwise, answer each of the following using PySpark / Spark SQL. For any queries you write make sure to include a `printSchema()` and sample(s) of the output which clearly demonstrates the code is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25feebd0-d09b-4233-b9af-ec965875930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo cp /home/jovyan/work/jars/neo4j-connector-apache-spark_2.12-4.1.0_for_spark_3.jar /usr/local/spark/jars/neo4j-connector-apache-spark_2.12-4.1.0_for_spark_3.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4308c98a-a418-4120-9ee7-05992d21e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q cassandra-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c73c8d6-8ab6-44a8-8a78-51598b8c07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12353bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR NAME ========>Soham Pendse\n",
    "# YOUR SU EMAIL ====> spendse@syr.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91718b91-ceae-416d-b96c-053a5d844676",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "In the cell below configure a spark session that is configured to connect to `mongodb`, `minio`, `cassandra`, '`elasticsearch` and `neo4j`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef5ecf9f-0798-4bc9-8896-229a0faade48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector-assembly_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "org.elasticsearch#elasticsearch-spark-20_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-df840673-7e5a-46b5-b4c6-faf2c7996e1a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-assembly_2.12;3.1.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.1.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.271 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "\tfound org.elasticsearch#elasticsearch-spark-20_2.12;7.15.0 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.6 in central\n",
      "\tfound commons-logging#commons-logging;1.1.1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.3.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound org.apache.spark#spark-yarn_2.12;2.4.4 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-assembly_2.12/3.1.0/spark-cassandra-connector-assembly_2.12-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-assembly_2.12;3.1.0!spark-cassandra-connector-assembly_2.12.jar (7438ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.1.2/hadoop-aws-3.1.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.1.2!hadoop-aws.jar (197ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (351ms)\n",
      "downloading https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-spark-20_2.12/7.15.0/elasticsearch-spark-20_2.12-7.15.0.jar ...\n",
      "\t[SUCCESSFUL ] org.elasticsearch#elasticsearch-spark-20_2.12;7.15.0!elasticsearch-spark-20_2.12.jar (649ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.271/aws-java-sdk-bundle-1.11.271.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.271!aws-java-sdk-bundle.jar (27815ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (77ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (149ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (395ms)\n",
      ":: resolution report :: resolve 12681ms :: artifacts dl 37133ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.271 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-assembly_2.12;3.1.0 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-yarn_2.12;2.4.4 from central in [default]\n",
      "\torg.elasticsearch#elasticsearch-spark-20_2.12;7.15.0 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.6 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   14  |   14  |   0   ||   8   |   8   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-df840673-7e5a-46b5-b4c6-faf2c7996e1a\n",
      "\tconfs: [default]\n",
      "\t8 artifacts copied, 0 already retrieved (103971kB/13330ms)\n",
      "23/05/01 16:31:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/01 16:32:29 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:524)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/05/01 16:32:29 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:78)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:589)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1000)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:524)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n",
      "23/05/01 16:32:34 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:524)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/05/01 16:32:34 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:78)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:589)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1000)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:524)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n"
     ]
    }
   ],
   "source": [
    "#1 Spark session\n",
    "minio_user = \"minio\"\n",
    "minio_password = \"SU2orange!\"\n",
    "minio_endpoint = \"http://minio:9000\"\n",
    "mongo_uri = f\"mongodb://admin:mongopw@mongo:27017/ischooldb?authSource=admin\"\n",
    "cassandra_host = \"cassandra\"\n",
    "elastic_host = \"elasticsearch\"\n",
    "elastic_port = \"9200\"\n",
    "bolt_url = \"bolt://neo4j:7687\"\n",
    "\n",
    "jars = [\n",
    "    \"com.datastax.spark:spark-cassandra-connector-assembly_2.12:3.1.0\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.1.2\",\n",
    "    \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\",\n",
    "    \"org.elasticsearch:elasticsearch-spark-20_2.12:7.15.0\"\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "        .config(\"spark.jars.packages\",\",\".join(jars) )\\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\",minio_endpoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\",minio_user) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", minio_password) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
    "        .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "        .config(\"spark.cassandra.connection.host\", cassandra_host) \\\n",
    "        .config(\"spark.es.nodes\", elastic_host) \\\n",
    "        .config(\"spark.es.port\",elastic_port) \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee055a5-7a8f-4739-a3f5-a52d981ea05e",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Demonstrate you can read the process-oriented data `enrollments` and `sections` from `minio` using PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50367be-c7ac-43b0-b820-54e4b7d83c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#2a enrollments\n",
    "enrollments = spark.read.option(\"inferSchema\",\"true\")\\\n",
    "                .option(\"header\",\"true\")\\\n",
    "                .option(\"ignoreLeadingWhiteSpace\", \"true\")\\\n",
    "                .csv(\"s3a://enrollments/enrollments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70d93b67-e5f8-4a9d-8902-11ff467edccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+------+-------+-----------+-----+------------+\n",
      "|term|course_enrollment|course|section| student_id|grade|grade_points|\n",
      "+----+-----------------+------+-------+-----------+-----+------------+\n",
      "|1221|                1|IST659|   M001|orenjouglad|    C|         2.0|\n",
      "|1221|                2|IST659|   M001|billmelator|    A|         4.0|\n",
      "|1221|                3|IST659|   M001| morrisless|    A|         4.0|\n",
      "+----+-----------------+------+-------+-----------+-----+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enrollments.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b415479-5acf-42dc-b535-c80bc62d239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2b sections \n",
    "sections = spark.read.option(\"inferSchema\",\"true\")\\\n",
    "                .option(\"header\",\"true\")\\\n",
    "                .option(\"ignoreLeadingWhiteSpace\", \"true\")\\\n",
    "                .csv(\"s3a://enrollments/sections.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b88c601b-6352-4624-bb0b-856a982ba6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+----------+--------+\n",
      "|term|course|section|enrollment|capacity|\n",
      "+----+------+-------+----------+--------+\n",
      "|1221|IST659|   M001|        20|      20|\n",
      "|1221|IST659|   M002|        20|      20|\n",
      "|1221|IST722|   M001|        25|      28|\n",
      "|1221|IST615|   M001|        22|      28|\n",
      "|1221|IST621|   M001|        22|      24|\n",
      "+----+------+-------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sections.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74906498-d775-451f-befd-5e88595b7009",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Demonstrate you can read the reference-oriented data `terms`, `students`, `courses`, and `program` reference data from `MongoDb` using PySpark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f928bc1-1306-42d7-a073-c7673dde4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3a terms \n",
    "terms = spark.read.format(\"mongo\")\\\n",
    "    .option(\"database\",\"ischooldb\")\\\n",
    "    .option(\"collection\",\"terms\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddc06fd2-b8f6-423b-a958-bbcb42920f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+----+-----------+--------+----+\n",
      "| _id|academic_year|code|       name|semester|year|\n",
      "+----+-------------+----+-----------+--------+----+\n",
      "|1221|    2021-2022|1221|  Fall 2021|    Fall|2021|\n",
      "|1222|    2021-2022|1222|Spring 2022|  Spring|2022|\n",
      "|1231|    2022-2023|1231|  Fall 2022|    Fall|2022|\n",
      "+----+-------------+----+-----------+--------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terms.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b917eaca-2e3e-45ab-84aa-bc2f07bf6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3b courses\n",
    "courses = spark.read.format(\"mongo\")\\\n",
    "    .option(\"database\",\"ischooldb\")\\\n",
    "    .option(\"collection\",\"courses\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f78ac0c-046f-4ca9-9e8c-aba4545fac9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q10' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86/873670289.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'q10' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f483a03-3da6-4efa-95d7-06c82e556400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3c Programs\n",
    "programs = spark.read.format(\"mongo\")\\\n",
    "    .option(\"database\",\"ischooldb\")\\\n",
    "    .option(\"collection\",\"programs\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a539d3a7-9706-43af-b3dc-4ab3dc59e3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+--------------------+--------------------+--------------------+-----------+\n",
      "|_id|code|credits|    elective_courses|                name|    required_courses|       type|\n",
      "+---+----+-------+--------------------+--------------------+--------------------+-----------+\n",
      "| IS|  IS|     36|[IST722, IST714, ...| Information Systems|[IST659, IST615, ...|    Masters|\n",
      "| DS|  DS|     34|    [IST769, IST714]|        Data Science|[IST659, IST615, ...|    Masters|\n",
      "|BDC| BDC|      9|                null|Data Engineering ...|[IST659, IST722, ...|Certificate|\n",
      "+---+----+-------+--------------------+--------------------+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "programs.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88d87294-61eb-4dd4-8b7a-5c280b6b7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d students\n",
    "students = spark.read.format(\"mongo\")\\\n",
    "    .option(\"database\",\"ischooldb\")\\\n",
    "    .option(\"collection\",\"students\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "462284f7-8566-498a-93c0-d9a2725bdb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------+\n",
      "|       _id|       name|program|\n",
      "+----------+-----------+-------+\n",
      "|  abbykuss|  Abby Kuss|     DS|\n",
      "|adamantium|Adam Antium|     IS|\n",
      "| addieowse| Addie Owse|     IS|\n",
      "+----------+-----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abfca28-8272-46ee-b98c-6a263be01c56",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Prepare the `section` data for loading into `cassandra` and `elasticsearch` with Spark or Spark SQL. Just PREPARE it do not LOAD it. Remember that we want this data to be as wide as possible, so include all relevant reference data. For example, the `section` data should include `term` attributes like `year`,  `academic year`, etc... and from `course`, attributes like `credits`, `name`, `prerequisites`, etc... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e8a9218-9dc1-4e69-87d8-90274d579ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 wide_sections\n",
    "section_load = sections.join(terms, sections.term == terms._id, \"left\").select(sections.term,sections.course,sections.section,sections.enrollment,sections.capacity,\n",
    "                                                                        terms.academic_year,terms.name,terms.semester,terms.year)\\\n",
    "                                                                        .withColumnRenamed(\"name\",\"term_name\")\\\n",
    "                .join(courses, sections.course == courses.code).drop(\"_id\",\"code\").withColumnRenamed(\"name\",\"course_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fd77f02-3cfb-4c58-8384-1ba7c0fbde1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:======================================================>(99 + 1) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+----------+--------+-------------+-----------+--------+----+-------+--------------------+--------------------+----------------+----------------+-------------+--------------------+\n",
      "|term|course|section|enrollment|capacity|academic_year|  term_name|semester|year|credits|         description|elective_in_programs| key_assignments|     course_name|prerequisites|required_in_programs|\n",
      "+----+------+-------+----------+--------+-------------+-----------+--------+----+-------+--------------------+--------------------+----------------+----------------+-------------+--------------------+\n",
      "|1232|IST615|   M001|        21|      28|    2022-2023|Spring 2023|  Spring|2023|      3|Cloud services cr...|                  []|[project, paper]|Cloud Management|           []|            [IS, DS]|\n",
      "|1232|IST615|   M002|        20|      24|    2022-2023|Spring 2023|  Spring|2023|      3|Cloud services cr...|                  []|[project, paper]|Cloud Management|           []|            [IS, DS]|\n",
      "|1222|IST615|   M001|        19|      24|    2021-2022|Spring 2022|  Spring|2022|      3|Cloud services cr...|                  []|[project, paper]|Cloud Management|           []|            [IS, DS]|\n",
      "+----+------+-------+----------+--------+-------------+-----------+--------+----+-------+--------------------+--------------------+----------------+----------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "section_load.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3bc55c1-42bd-4cca-b8de-1b17de1a7e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- term: integer (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- section: string (nullable = true)\n",
      " |-- enrollment: integer (nullable = true)\n",
      " |-- capacity: integer (nullable = true)\n",
      " |-- academic_year: string (nullable = true)\n",
      " |-- term_name: string (nullable = true)\n",
      " |-- semester: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- credits: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- elective_in_programs: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- key_assignments: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- course_name: string (nullable = true)\n",
      " |-- prerequisites: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- required_in_programs: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "section_load.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f972cf-8ab1-48fe-b2ff-607acc11e05d",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Use the `cassandra-driver` example from class to write python code to connect to cassandra from within Jupyter and create a keyspace named `ischooldb`. Design a cassandra table called `sections` to store the data from question 4. Appropriate key design is important! Please explain your justification for key below your table definition. Provide clear evidence that your table was created by querying the empty table in spark and use `printSchema()` to show the schema. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4015e68d-c1d0-4bb6-962e-f42bdaeb28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 create cassandra table for wide_sections\n",
    "from cassandra.cluster import Cluster\n",
    "with Cluster([cassandra_host]) as cluster:\n",
    "    session = cluster.connect()\n",
    "    session.execute(\"CREATE KEYSPACE IF NOT EXISTS ischooldb WITH replication={ 'class': 'SimpleStrategy', 'replication_factor' : 1 };\")\n",
    "    table = '''\n",
    "    CREATE TABLE IF NOT EXISTS ischooldb.sections (\n",
    "        term text,\n",
    "        course text,\n",
    "        section text,\n",
    "        enrollment int,\n",
    "        capacity int,\n",
    "        academic_year text,\n",
    "        term_name text,\n",
    "        semester text,\n",
    "        year text,\n",
    "        credits int,\n",
    "        description text, \n",
    "        elective_in_programs list<text> ,\n",
    "        key_assignments list<text>,\n",
    "        course_name text,\n",
    "        prerequisites list<text>,\n",
    "        required_in_programs list<text>,\n",
    "    primary key ((term, course,section), year) \n",
    "    );\n",
    "    '''\n",
    "    session.execute(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb1629ce-e050-44e7-8e2c-4874e00dd198",
   "metadata": {},
   "outputs": [],
   "source": [
    "q4_emptytable =spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"sections\", keyspace=\"ischooldb\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "241480df-a969-4152-a4d7-4fa315e45392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- term: string (nullable = false)\n",
      " |-- course: string (nullable = false)\n",
      " |-- section: string (nullable = false)\n",
      " |-- year: string (nullable = true)\n",
      " |-- academic_year: string (nullable = true)\n",
      " |-- capacity: integer (nullable = true)\n",
      " |-- course_name: string (nullable = true)\n",
      " |-- credits: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- elective_in_programs: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- enrollment: integer (nullable = true)\n",
      " |-- key_assignments: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- prerequisites: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- required_in_programs: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- semester: string (nullable = true)\n",
      " |-- term_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q4_emptytable.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baccf8f1-6450-44b5-9389-9b4532356be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+----+-------------+--------+--------------------+-------+--------------------+--------------------+----------+----------------+-------------+--------------------+--------+-----------+\n",
      "|term|course|section|year|academic_year|capacity|         course_name|credits|         description|elective_in_programs|enrollment| key_assignments|prerequisites|required_in_programs|semester|  term_name|\n",
      "+----+------+-------+----+-------------+--------+--------------------+-------+--------------------+--------------------+----------+----------------+-------------+--------------------+--------+-----------+\n",
      "|1232|IST615|   M001|2023|    2022-2023|      28|    Cloud Management|      3|Cloud services cr...|                  []|        21|[project, paper]|           []|            [IS, DS]|  Spring|Spring 2023|\n",
      "|1222|IST687|   M001|2022|    2021-2022|      20|Introduction to D...|      3|Introduces inform...|                [IS]|        18| [project, exam]|           []|                [DS]|  Spring|Spring 2022|\n",
      "|1232|IST769|   M001|2023|    2022-2023|      24|Advanced Big Data...|      3|Analyze relationa...|                [DS]|        20| [project, exam]|     [IST659]|                  []|  Spring|Spring 2023|\n",
      "|1222|IST659|   M001|2022|    2021-2022|      24|Data Administrati...|      3|Definition, devel...|                  []|        24|       [project]|           []|            [IS, DS]|  Spring|Spring 2022|\n",
      "|1222|IST714|   M001|2022|    2021-2022|      20|  Cloud Architecture|      3|Advanced, lab-bas...|            [IS, DS]|        17|       [project]|     [IST615]|                  []|  Spring|Spring 2022|\n",
      "+----+------+-------+----+-------------+--------+--------------------+-------+--------------------+--------------------+----------+----------------+-------------+--------------------+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "q4_emptytable.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee71b3b5-03d7-456d-a64f-f3c5f0c93836",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Load the data frame you created in question 4 into the `cassandra` table you created in question 5. Demonstrate the data is in the table by querying back it with PySpark. Make sure you can run the code multiple times and each time it replaces the existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59e9aac0-da49-4c82-8e3f-184b5f1d9eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#6 load wide_sections into cassandra\n",
    "section_load.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "  .mode(\"Append\")\\\n",
    "  .option(\"table\", \"sections\")\\\n",
    "  .option(\"keyspace\",\"ischooldb\")\\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc379d83-b138-4d2f-97a5-4fa6676181a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_cassandra = spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "                    .options(table=\"sections\", keyspace=\"ischooldb\") \\\n",
    "                    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3a19a2d-5353-421c-983a-4bd2532942af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+----+-------------+--------+--------------------+-------+--------------------+--------------------+----------+----------------+-------------+--------------------+--------+-----------+\n",
      "|term|course|section|year|academic_year|capacity|         course_name|credits|         description|elective_in_programs|enrollment| key_assignments|prerequisites|required_in_programs|semester|  term_name|\n",
      "+----+------+-------+----+-------------+--------+--------------------+-------+--------------------+--------------------+----------+----------------+-------------+--------------------+--------+-----------+\n",
      "|1222|IST615|   M001|2022|    2021-2022|      24|    Cloud Management|      3|Cloud services cr...|                  []|        19|[project, paper]|           []|            [IS, DS]|  Spring|Spring 2022|\n",
      "|1231|IST615|   M001|2022|    2022-2023|      24|    Cloud Management|      3|Cloud services cr...|                  []|        21|[project, paper]|           []|            [IS, DS]|    Fall|  Fall 2022|\n",
      "|1222|IST687|   M002|2022|    2021-2022|      20|Introduction to D...|      3|Introduces inform...|                [IS]|        20| [project, exam]|           []|                [DS]|  Spring|Spring 2022|\n",
      "|1221|IST687|   M001|2021|    2021-2022|      20|Introduction to D...|      3|Introduces inform...|                [IS]|        20| [project, exam]|           []|                [DS]|    Fall|  Fall 2021|\n",
      "|1232|IST615|   M002|2023|    2022-2023|      24|    Cloud Management|      3|Cloud services cr...|                  []|        20|[project, paper]|           []|            [IS, DS]|  Spring|Spring 2023|\n",
      "|1232|IST621|   M002|2023|    2022-2023|      24|Information Manag...|      3|Information and t...|                  []|        21|         [paper]|           []|                [IS]|  Spring|Spring 2023|\n",
      "|1231|IST687|   M001|2022|    2022-2023|      20|Introduction to D...|      3|Introduces inform...|                [IS]|        17| [project, exam]|           []|                [DS]|    Fall|  Fall 2022|\n",
      "|1221|IST621|   M001|2021|    2021-2022|      24|Information Manag...|      3|Information and t...|                  []|        22|         [paper]|           []|                [IS]|    Fall|  Fall 2021|\n",
      "|1231|IST707|   M001|2022|    2022-2023|      24|Applied Machine L...|      3|General overview ...|                [IS]|        24|          [exam]|     [IST687]|                [DS]|    Fall|  Fall 2022|\n",
      "|1231|IST659|   M001|2022|    2022-2023|      20|Data Administrati...|      3|Definition, devel...|                  []|        20|       [project]|           []|            [IS, DS]|    Fall|  Fall 2022|\n",
      "+----+------+-------+----+-------------+--------+--------------------+-------+--------------------+--------------------+----------+----------------+-------------+--------------------+--------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "section_cassandra.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052fbcc-b7cc-4fa7-ba5d-09b3066c8b96",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Since we did not learn how to create a custom elasticsearch mapping, before you can load the data into `elasticsearch` you will need to flatten the nested data. For example, `course_is_elective_in_programs` should generate 2 columns `course_is_elective_for_IS` and `course_is_elective_for_DS`. You'll need to repeat this step for `course_is_required_in_programs`. Omit the `course_prerequisites` and `course_key_assignments` column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99f91002-2880-4f4c-8392-9b869ce42172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 flatten `course_is_elective_in_programs` and `course_is_required_in_programs` \n",
    "from pyspark.sql.functions import explode_outer, col, when, flatten\n",
    "course_is_elective_in_programs = section_cassandra.select(\"term\",\"course\",\"section\",\"year\",\"required_in_programs\",explode_outer(section_cassandra.elective_in_programs))\\\n",
    "                                    .withColumnRenamed(\"col\",\"elective_in_course\").drop(\"null\")\n",
    "course_is_required_in_programs = section_cassandra.select(\"term\",\"course\",\"section\",\"year\",explode_outer(section_cassandra.required_in_programs))\\\n",
    "                                    .withColumnRenamed(\"col\",\"required_programs\").drop(\"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bfe1d01-87bf-41f6-9453-ae067601ef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "course_is_elective_in_programs = course_is_elective_in_programs.groupBy(\"term\",\"course\",\"section\",\"year\").pivot(\"elective_in_course\").count()\\\n",
    "                                .withColumnRenamed(\"DS\",\"elective_ds\").withColumnRenamed(\"IS\",\"elective_is\").na.fill(value = 0, subset = [\"elective_ds\",\"elective_is\"]).drop(\"null\")\n",
    "course_is_required_in_programs = course_is_required_in_programs.groupBy(\"term\",\"course\",\"section\",\"year\").pivot(\"required_programs\").count()\\\n",
    "                                .withColumnRenamed(\"DS\",\"required_ds\").withColumnRenamed(\"IS\",\"required_is\")\\\n",
    "                                .withColumnRenamed(\"term\",\"required_term\").withColumnRenamed(\"course\",\"required_course\").withColumnRenamed(\"section\",\"required_section\")\\\n",
    "                                .withColumnRenamed(\"year\",\"required_year\").na.fill(value = 0, subset = [\"required_ds\",\"required_is\"]).drop(\"null\")\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e2eeaf6-24c1-4fbe-a522-ae95ac152484",
   "metadata": {},
   "outputs": [],
   "source": [
    "q7 = course_is_elective_in_programs.join(course_is_required_in_programs,(course_is_elective_in_programs.term == course_is_required_in_programs.required_term ) &\n",
    "                                    (course_is_elective_in_programs.course == course_is_required_in_programs.required_course ) &\n",
    "                                    (course_is_elective_in_programs.section == course_is_required_in_programs.required_section ),\"inner\").drop(\"term\",\"course\",\"section\",\"year\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "054dd6b1-fa16-4192-90f0-5e0406e227fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "q7_load = q7.join(section_cassandra, (section_cassandra.term == q7.required_term) & (section_cassandra.course == q7.required_course) & (section_cassandra.section == q7.required_section))\\\n",
    "    .drop(\"elective_in_programs\",\"required_in_programs\",\"key_assignments\",\"prerequisites\", \"required_year\", \"required_term\",\"required_section\", \"required_course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f023884-1985-4b36-a7a4-ae2b3a2dccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+-----------+----+------+-------+----+-------------+--------+--------------------+-------+--------------------+----------+--------+-----------+\n",
      "|elective_ds|elective_is|required_ds|required_is|term|course|section|year|academic_year|capacity|         course_name|credits|         description|enrollment|semester|  term_name|\n",
      "+-----------+-----------+-----------+-----------+----+------+-------+----+-------------+--------+--------------------+-------+--------------------+----------+--------+-----------+\n",
      "|          0|          1|          0|          0|1231|IST722|   M001|2022|    2022-2023|      28|    Data Warehousing|      3|Introduction to c...|        23|    Fall|  Fall 2022|\n",
      "|          0|          0|          0|          1|1222|IST621|   M001|2022|    2021-2022|      28|Information Manag...|      3|Information and t...|        28|  Spring|Spring 2022|\n",
      "|          0|          1|          1|          0|1231|IST707|   M001|2022|    2022-2023|      24|Applied Machine L...|      3|General overview ...|        24|    Fall|  Fall 2022|\n",
      "+-----------+-----------+-----------+-----------+----+------+-------+----+-------------+--------+--------------------+-------+--------------------+----------+--------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q7_load.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd3d88-efe9-4d0c-90ce-941ef6de84e2",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Load the data frame you created in question 7 into `elasticsearch`, under the index `sections`.  Demonstrate the data is in the index by querying back it with PySpark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39d2c484-7857-4064-a60c-c1ec0004b370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#8 load wide_sections_flattened into elasticsearch\n",
    "q7_load.write.mode(\"Overwrite\").format(\"es\").save(\"sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95ed0055-56d8-41d2-9064-0f9b6e8b1574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+------+--------------------+-------+--------------------+-----------+-----------+----------+-----------+-----------+---------------+--------+------------+-----------+----+\n",
      "|academic_year|capacity|course|         course_name|credits|         description|elective_ds|elective_is|enrollment|required_ds|required_is|offered_section|semester|offered_term|  term_name|year|\n",
      "+-------------+--------+------+--------------------+-------+--------------------+-----------+-----------+----------+-----------+-----------+---------------+--------+------------+-----------+----+\n",
      "|    2021-2022|      28|IST707|Applied Machine L...|      3|General overview ...|          0|          1|        28|          1|          0|           M001|    Fall|        1221|  Fall 2021|2021|\n",
      "|    2021-2022|      28|IST621|Information Manag...|      3|Information and t...|          0|          0|        28|          0|          1|           M001|  Spring|        1222|Spring 2022|2022|\n",
      "|    2022-2023|      28|IST718|  Big Data Analytics|      3|A broad introduct...|          0|          0|        28|          1|          0|           M001|  Spring|        1232|Spring 2023|2023|\n",
      "|    2021-2022|      28|IST718|  Big Data Analytics|      3|A broad introduct...|          0|          0|        28|          1|          0|           M001|  Spring|        1222|Spring 2022|2022|\n",
      "|    2021-2022|      28|IST722|    Data Warehousing|      3|Introduction to c...|          0|          1|        25|          0|          0|           M001|    Fall|        1221|  Fall 2021|2021|\n",
      "+-------------+--------+------+--------------------+-------+--------------------+-----------+-----------+----------+-----------+-----------+---------------+--------+------------+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q8 = spark.read.format(\"es\").load(\"sections\").withColumnRenamed(\"term\",\"offered_term\").withColumnRenamed(\"section\",\"offered_section\")\n",
    "q8.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa738b4b-6970-46d4-b5dc-3c766f7fe64b",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Similar to question 4, prepare the `enrollments` for loading into `cassandra` and `elasticsearch` with Spark or Spark SQL. For this wide table we want to include the same reference data for `sections` but include the `student` attributes and the `program` data associated with the student. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce4ea5ef-282b-4aca-86c3-5401ef105088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 create wide_enrollments\n",
    "student_program = students.withColumnRenamed(\"_id\",\"students_student_id\").withColumnRenamed(\"name\",\"student_name\")\\\n",
    "                    .join(programs, students.program == programs._id).drop(\"code\",\"_id\")\n",
    "enrollments_load = student_program.join(enrollments, student_program.students_student_id == enrollments.student_id).drop(\"student_id\")\\\n",
    "                    .withColumnRenamed(\"course\", \"enrolled_course\").withColumnRenamed(\"credits\",\"program_credits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "159b2d0d-d735-47dd-9013-9f91b96b900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrollments_cassandra = enrollments_load.join(q8, (enrollments_load.enrolled_course == q8.course)&\n",
    "                                              (enrollments_load.term == q8.offered_term)&\n",
    "                                              (enrollments_load.section == q8.offered_section),\"inner\").withColumnRenamed(\"name\", \"program_name\").drop(\"offered_term\",\"offered_section\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87e61f83-77ef-49b9-80fd-15f0371776df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- students_student_id: string (nullable = true)\n",
      " |-- student_name: string (nullable = true)\n",
      " |-- program: string (nullable = true)\n",
      " |-- program_credits: integer (nullable = true)\n",
      " |-- elective_courses: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- program_name: string (nullable = true)\n",
      " |-- required_courses: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- term: integer (nullable = true)\n",
      " |-- course_enrollment: integer (nullable = true)\n",
      " |-- enrolled_course: string (nullable = true)\n",
      " |-- section: string (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- grade_points: double (nullable = true)\n",
      " |-- academic_year: string (nullable = true)\n",
      " |-- capacity: long (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- course_name: string (nullable = true)\n",
      " |-- credits: long (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- elective_ds: long (nullable = true)\n",
      " |-- elective_is: long (nullable = true)\n",
      " |-- enrollment: long (nullable = true)\n",
      " |-- required_ds: long (nullable = true)\n",
      " |-- required_is: long (nullable = true)\n",
      " |-- semester: string (nullable = true)\n",
      " |-- term_name: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enrollments_cassandra.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09109136-baa3-420c-8c08-9ec09e3cf38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "with Cluster([cassandra_host]) as cluster:\n",
    "    session = cluster.connect()\n",
    "    session.execute(\"CREATE KEYSPACE IF NOT EXISTS ischooldb WITH replication={ 'class': 'SimpleStrategy', 'replication_factor' : 1 };\")\n",
    "    table = '''\n",
    "    CREATE TABLE IF NOT EXISTS ischooldb.enrollments (\n",
    "        students_student_id text,\n",
    "        student_name text,\n",
    "        program text,\n",
    "        elective_courses list<text>,\n",
    "        program_name text,\n",
    "        required_courses list<text>,\n",
    "        type text,\n",
    "        term int,\n",
    "        course_enrollment int,\n",
    "        enrolled_course text,\n",
    "        grade text,\n",
    "        grade_points double,\n",
    "        academic_year text,\n",
    "        capacity int,\n",
    "        section text,\n",
    "        course text,\n",
    "        course_name text,\n",
    "        credits int,\n",
    "        description text,\n",
    "        elective_DS text,\n",
    "        elective_IS text,\n",
    "        required_DS text,\n",
    "        required_IS text,\n",
    "        semester text,\n",
    "        year text,\n",
    "        term_name text,\n",
    "        enrollment int,\n",
    "        program_credits int,\n",
    "    primary key ((students_student_id,course,section,term), program) \n",
    "    );\n",
    "    '''\n",
    "    session.execute(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81b60492-4665-4d4d-a356-123254af47de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "enrollments_cassandra.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "  .mode(\"Append\")\\\n",
    "  .option(\"table\", \"enrollments\")\\\n",
    "  .option(\"keyspace\",\"ischooldb\")\\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4631726-4092-49f7-8280-f1cb77c90ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassandra_enrollment = spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "                    .options(table=\"enrollments\", keyspace=\"ischooldb\") \\\n",
    "                    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "364d2a1e-b049-441c-8407-3b13143aa784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-------+----+-------+-------------+--------+-----------------+--------------------+-------+--------------------+--------------------+-----------+-----------+---------------+----------+-----+------------+---------------+-------------------+--------------------+-----------+-----------+--------+------------------+-----------+-------+----+\n",
      "|students_student_id|course|section|term|program|academic_year|capacity|course_enrollment|         course_name|credits|         description|    elective_courses|elective_ds|elective_is|enrolled_course|enrollment|grade|grade_points|program_credits|       program_name|    required_courses|required_ds|required_is|semester|      student_name|  term_name|   type|year|\n",
      "+-------------------+------+-------+----+-------+-------------+--------+-----------------+--------------------+-------+--------------------+--------------------+-----------+-----------+---------------+----------+-----+------------+---------------+-------------------+--------------------+-----------+-----------+--------+------------------+-----------+-------+----+\n",
      "|  dorisnolongeropen|IST659|   M002|1221|     DS|    2021-2022|      20|               15|Data Administrati...|      3|Definition, devel...|    [IST769, IST714]|          0|          0|         IST659|        20|    C|         2.0|             34|       Data Science|[IST659, IST615, ...|          1|          1|    Fall|Doris Nolongeropen|  Fall 2021|Masters|2021|\n",
      "|         sherrywyne|IST659|   M001|1222|     DS|    2021-2022|      24|               20|Data Administrati...|      3|Definition, devel...|    [IST769, IST714]|          0|          0|         IST659|        24|   A-|       3.667|             34|       Data Science|[IST659, IST615, ...|          1|          1|  Spring|       Sherry Wyne|Spring 2022|Masters|2022|\n",
      "|        mikerophone|IST687|   M001|1232|     IS|    2022-2023|      24|                7|Introduction to D...|      3|Introduces inform...|[IST722, IST714, ...|          0|          1|         IST687|        19|    A|         4.0|             36|Information Systems|[IST659, IST615, ...|          1|          0|  Spring|      Mike Rophone|Spring 2023|Masters|2023|\n",
      "+-------------------+------+-------+----+-------+-------------+--------+-----------------+--------------------+-------+--------------------+--------------------+-----------+-----------+---------------+----------+-----+------------+---------------+-------------------+--------------------+-----------+-----------+--------+------------------+-----------+-------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cassandra_enrollment.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae284c8-096a-4987-98cf-0800cedced12",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "Load the data frame you created in question 8 into `elasticsearch`, under the index `enrollments`. This time, just Omit all array types to make the problem simpler (`elective_courses`, `key_assignments`, `course_prerequisites`, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a42dedc-62c4-4b83-b431-2a3474834901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 wide_enrollments to elastic search\n",
    "q10 = cassandra_enrollment.drop(\"required_courses\",\"elective_courses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c41503c-fad3-4b5d-935c-2c0c73d3ebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "q10.write.mode(\"Overwrite\").format(\"es\").save(\"enrollments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d949430d-e5d2-4a2c-9bae-1bc72ee1118d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+------+-----------------+--------------------+-------+--------------------+-----------+-----------+---------------+----------+-----+------------+-------+---------------+-------------------+-----------+-----------+-------+--------+------------+-------------------+----+-----------+-------+----+\n",
      "|academic_year|capacity|course|course_enrollment|         course_name|credits|         description|elective_ds|elective_is|enrolled_course|enrollment|grade|grade_points|program|program_credits|       program_name|required_ds|required_is|section|semester|student_name|students_student_id|term|  term_name|   type|year|\n",
      "+-------------+--------+------+-----------------+--------------------+-------+--------------------+-----------+-----------+---------------+----------+-----+------------+-------+---------------+-------------------+-----------+-----------+-------+--------+------------+-------------------+----+-----------+-------+----+\n",
      "|    2022-2023|      24|IST707|                5|Applied Machine L...|      3|General overview ...|          0|          1|         IST707|        24|    A|         4.0|     IS|             36|Information Systems|          1|          0|   M001|    Fall|Lee Hvmeehom|        leehvmeehom|1231|  Fall 2022|Masters|2022|\n",
      "|    2021-2022|      24|IST621|               18|Information Manag...|      3|Information and t...|          0|          0|         IST621|        22|   A-|       3.667|     IS|             36|Information Systems|          0|          1|   M001|    Fall|Eura Quittin|        euraquittin|1221|  Fall 2021|Masters|2021|\n",
      "|    2022-2023|      24|IST769|                5|Advanced Big Data...|      3|Analyze relationa...|          1|          0|         IST769|        20|   A-|       3.667|     DS|             34|       Data Science|          0|          0|   M001|  Spring|  Ty Itdowne|          tyitdowne|1232|Spring 2023|Masters|2023|\n",
      "+-------------+--------+------+-----------------+--------------------+-------+--------------------+-----------+-----------+---------------+----------+-----+------------+-------+---------------+-------------------+-----------+-----------+-------+--------+------------+-------------------+----+-----------+-------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df10 = spark.read.format(\"es\").load(\"enrollments\")\n",
    "df10.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40baf832-3a14-45c9-9594-d607439b845a",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "Write spark to clear the `neo4j` database of all nodes and relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd7a903e-789b-4b01-8501-6041190b50c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#11 reset neo4j database \n",
    "cipher_ql = '''\n",
    "MATCH (n)\n",
    "DETACH DELETE n\n",
    "'''\n",
    "df = spark.createDataFrame(data = [{'row':1}])\n",
    "df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Overwrite\") \\\n",
    "  .option(\"url\", bolt_url) \\\n",
    "  .option(\"query\",cipher_ql) \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6910199-edde-418d-b6fa-06c5d810ce3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 12\n",
    "\n",
    "Load the `courses` and `program` data into `neo4j` as nodes. Exclude the `requirements`, `electives` and `prerequisites` from the node attributes. Demonstrate the data in `neo4j` by querying back it using one or more Cypher queries. NOTE: the Neo4J `name` attribute is what will display on the node bubbles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71eeb25c-fa18-4702-9595-34f443aca315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#12a load courses into Neo4j\n",
    "cipher_ql = '''Create (c:Course {course_name: event.name, code: event.code, course_id: event._id, credits: event.credits, description: event.description,\n",
    "                    key_assignments:event.key_assignments, prerequisites: event.prerequisites})'''\n",
    "courses.select(\"code\",\"_id\",\"credits\",\"description\",\"name\",\"key_assignments\",\"prerequisites\")\\\n",
    "    .write.format(\"org.neo4j.spark.DataSource\").mode(\"Overwrite\") \\\n",
    "      .option(\"url\", bolt_url) \\\n",
    "      .option(\"query\",cipher_ql) \\\n",
    "      .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69aa723b-5479-4232-aca3-0197e199cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12b load programs into neo4j\n",
    "cipher_ql = '''CREATE (p:Program {program_name: event.name, code: event.code, program_id: event._id, credits: event.credits, type: event.type, \n",
    "                        required_courses: event.required_courses, elective_courses:event.elective_courses})'''\n",
    "programs.drop(\"elective_course\").write.format(\"org.neo4j.spark.DataSource\").mode(\"Overwrite\") \\\n",
    "      .option(\"url\", bolt_url) \\\n",
    "      .option(\"query\",cipher_ql) \\\n",
    "      .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5508275-5202-43f5-8adf-773dc22fc681",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "Load the `requirements` and `electives` data into `neo4j` as relationships to the nodes you created in Question 12. Use the `program` data to form the `required` and `elective` course relationships. Demonstrate the relationships in `neo4j` are present by querying back it using one or more Cypher queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f782d89-fa86-4c79-b1f0-baf383c47100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13a program course requirements\n",
    "cipher_ql = '''Match(p:Program),(c:Course)\n",
    "WHERE Any (course in c.course_id Where course in p.required_courses)\n",
    "Merge (p)- [r:required] -> (c)\n",
    "return p, c, r\n",
    "'''\n",
    "df = spark.createDataFrame(data = [{'row':1}])\n",
    "df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Overwrite\") \\\n",
    "  .option(\"url\", bolt_url) \\\n",
    "  .option(\"query\",cipher_ql) \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9abc97bf-a75a-48d9-902a-aae259211e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13b program course electives\n",
    "cipher_ql = '''Match(p:Program),(c:Course)\n",
    "WHERE Any (course in c.course_id Where course in p.elective_courses)\n",
    "Merge (p)- [e:elective] -> (c)\n",
    "return p, c, e\n",
    "'''\n",
    "df = spark.createDataFrame(data = [{'row':1}])\n",
    "df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Overwrite\") \\\n",
    "  .option(\"url\", bolt_url) \\\n",
    "  .option(\"query\",cipher_ql) \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad62a7-505a-4bb8-a2bd-3756cf6719d5",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "\n",
    "Load the `prerequisites` into `neo4j` as relationships to the `course` nodes you created in Question 12. Demonstrate the relationships in `neo4j` are present by querying back it using one or more Cypher queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b9e586b-c5be-4908-bc31-26ab6ef5ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14 course prerequisites \n",
    "cipher_ql = '''Match(c1:Course),(c2:Course)\n",
    "WHERE Any (course in c1.course_id Where course in c2.prerequisites)\n",
    "Merge (c1) <- [p:prerequisite] -(c2)\n",
    "return p, c1, c2\n",
    "'''\n",
    "df = spark.createDataFrame(data = [{'row':1}])\n",
    "df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Overwrite\") \\\n",
    "  .option(\"url\", bolt_url) \\\n",
    "  .option(\"query\",cipher_ql) \\\n",
    "  .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4d61d-5076-4d6e-9f54-03e437d18333",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "\n",
    "Write a Cypher query to display courses which are required by both the `IS` and `DS` programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2252e315-eaba-47ee-9f8e-11fc43529c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15 Cypher query courses required in DS and IS\n",
    "cipher_ql = ''' \n",
    "Match(p1:Program), (c1:Course)\n",
    "WHERE Any (course in c1.course_id Where course in p1.required_courses) and p1.program_id = \"IS\" \n",
    "Match(p2:Program), (c2:Course)\n",
    "WHERE Any (course in c2.course_id Where course in p2.required_courses) and p2.program_id = \"DS\"     \n",
    "Merge (p1) - [r1:required_is] -> (c1)\n",
    "Merge (p2) - [r2:required_ds] -> (c2)\n",
    "return p1, r1, c1, p2,r2,c2\n",
    "'''\n",
    "df = spark.createDataFrame(data = [{'row':1}])\n",
    "df.write.format(\"org.neo4j.spark.DataSource\").mode(\"Overwrite\") \\\n",
    "  .option(\"url\", bolt_url) \\\n",
    "  .option(\"query\",cipher_ql) \\\n",
    "  .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe6590-97e2-477f-b860-7ffd5fe63640",
   "metadata": {},
   "source": [
    "### Question 16\n",
    "\n",
    "Write a Cypher query to retrieve the `course code`, `course title`, and the count of programs the course is a requirement in. Write as a Cypher query but retrieve the  output as a Spark Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b68985d-c57c-42b3-87a5-4b9c4b6114f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------+\n",
      "|c.code|       c.course_name|program_count|\n",
      "+------+--------------------+-------------+\n",
      "|IST659|Data Administrati...|            3|\n",
      "|IST722|    Data Warehousing|            1|\n",
      "|IST769|Advanced Big Data...|            1|\n",
      "|IST615|    Cloud Management|            3|\n",
      "|IST714|  Cloud Architecture|            1|\n",
      "+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#16 Cypher to spark table\n",
    "cipher_ql = '''\n",
    "MATCH (p:Program)-[r:required]->(c:Course)\n",
    "RETURN c.code, c.course_name, count(p) AS program_count\n",
    "'''\n",
    "df = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"url\", bolt_url) \\\n",
    "  .option(\"query\",cipher_ql) \\\n",
    "  .load()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bc633",
   "metadata": {},
   "source": [
    "### Questions 17,18,19 and 20\n",
    "\n",
    "These are not spark questions as they use kibana."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
